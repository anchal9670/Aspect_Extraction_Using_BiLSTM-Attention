{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CugIGuF38LsJ",
        "outputId": "dd49b576-5ae3-4462-920a-c15b1d1d849d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import string\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "import json\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.utils import to_categorical\n",
        "from keras.models import Sequential\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.optimizers import Adam\n",
        "from keras.callbacks import EarlyStopping\n",
        "import gensim.downloader as api\n",
        "from keras.models import Model\n",
        "from keras.layers import Embedding, Bidirectional, LSTM, Dense, TimeDistributed, Dropout, Input, Concatenate, Dot, Activation, multiply\n",
        "from sklearn.metrics import classification_report\n",
        "from gensim.models import KeyedVectors\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "from keras.regularizers import l2\n",
        "from sklearn.utils.class_weight import compute_class_weight\n",
        "from sklearn.metrics import classification_report, accuracy_score"
      ],
      "metadata": {
        "id": "cW1cSzQ38cAO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "nltk.download('averaged_perceptron_tagger')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "277VCn2z9LHe",
        "outputId": "d7b4653a-ef61-4cd6-ca09-da76c8c4ce6d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "word_vectors = api.load('glove-wiki-gigaword-200')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7nVGqW4Y9Ps3",
        "outputId": "d108b99a-09b6-4098-eb56-8ce70aab677a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[==================================================] 100.0% 252.1/252.1MB downloaded\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def process_data(data):\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    punctuation = set(string.punctuation)\n",
        "\n",
        "    tokens_list = []\n",
        "    aspects_list = []\n",
        "    bio_tags_list = []\n",
        "    token_length_list = []\n",
        "    pos_tags_list = []\n",
        "\n",
        "    for entry in data:\n",
        "        tokens = [token for token in entry[\"token\"] if token not in punctuation]\n",
        "        aspects = entry.get(\"aspects\", [])\n",
        "        pos_tags = nltk.pos_tag(tokens)\n",
        "\n",
        "        unique_aspects = []\n",
        "        bio_tags = ['O'] * len(tokens)\n",
        "\n",
        "        for aspect in aspects:\n",
        "            term = [char for char in aspect.get(\"term\", []) if char not in punctuation]\n",
        "            unique_aspects.append(''.join(term))\n",
        "\n",
        "            for i in range(len(tokens)):\n",
        "                if tokens[i:i + len(term)] == term:\n",
        "                    if i > 0 and bio_tags[i - 1] == 'B':\n",
        "                        bio_tags[i] = 'I'\n",
        "                    else:\n",
        "                        bio_tags[i] = 'B'\n",
        "                    if len(term) > 1:\n",
        "                        bio_tags[i + 1:i + len(term)] = ['I'] * (len(term) - 1)\n",
        "\n",
        "        filtered_tokens = tokens\n",
        "        filtered_bio_tags = bio_tags\n",
        "        filtered_pos_tags = [tag[1] for tag in pos_tags]\n",
        "\n",
        "        tokens_list.append(filtered_tokens)\n",
        "        aspects_list.append(unique_aspects)\n",
        "        bio_tags_list.append(filtered_bio_tags)\n",
        "        token_length_list.append([len(token) for token in filtered_tokens])\n",
        "        pos_tags_list.append(filtered_pos_tags)\n",
        "\n",
        "    df_result = pd.DataFrame({\n",
        "        'token': tokens_list,\n",
        "        'aspect': aspects_list,\n",
        "        'bioTag': bio_tags_list,\n",
        "        'pos': pos_tags_list,  # Add POS tags as a new column\n",
        "    })\n",
        "\n",
        "    return df_result\n"
      ],
      "metadata": {
        "id": "0unveJij9Ts_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load All Dataset\n",
        "laptop_train_json_file_path = '/content/drive/MyDrive/Common files/Common files/Dataset/Laptops/train.json'\n",
        "with open(laptop_train_json_file_path, 'r') as file:\n",
        "    laptop_train_data = json.load(file)\n",
        "laptop_test_json_file_path = '/content/drive/MyDrive/Common files/Common files/Dataset/Laptops/test.json'\n",
        "with open(laptop_test_json_file_path, 'r') as file:\n",
        "    laptop_test_data = json.load(file)\n",
        "laptop_valid_json_file_path = '/content/drive/MyDrive/Common files/Common files/Dataset/Laptops/valid.json'\n",
        "with open(laptop_valid_json_file_path, 'r') as file:\n",
        "    laptop_valid_data = json.load(file)\n",
        "\n",
        "\n",
        "mams_train_json_file_path = '/content/drive/MyDrive/Common files/Common files/Dataset/MAMS/train.json'\n",
        "with open(mams_train_json_file_path, 'r') as file:\n",
        "    mams_train_data = json.load(file)\n",
        "mams_test_json_file_path = '/content/drive/MyDrive/Common files/Common files/Dataset/MAMS/test.json'\n",
        "with open(mams_test_json_file_path, 'r') as file:\n",
        "    mams_test_data = json.load(file)\n",
        "mams_valid_json_file_path = '/content/drive/MyDrive/Common files/Common files/Dataset/MAMS/valid.json'\n",
        "with open(mams_valid_json_file_path, 'r') as file:\n",
        "    mams_valid_data = json.load(file)\n",
        "\n",
        "\n",
        "res_train_json_file_path = '/content/drive/MyDrive/Common files/Common files/Dataset/Restaurants/train.json'\n",
        "with open(res_train_json_file_path, 'r') as file:\n",
        "    res_train_data = json.load(file)\n",
        "res_test_json_file_path = '/content/drive/MyDrive/Common files/Common files/Dataset/Restaurants/test.json'\n",
        "with open(res_test_json_file_path, 'r') as file:\n",
        "    res_test_data = json.load(file)\n",
        "res_valid_json_file_path = '/content/drive/MyDrive/Common files/Common files/Dataset/Restaurants/valid.json'\n",
        "with open(res_valid_json_file_path, 'r') as file:\n",
        "    res_valid_data = json.load(file)\n",
        "\n",
        "\n",
        "tweet_train_json_file_path = '/content/drive/MyDrive/Common files/Common files/Dataset/Tweets/train.json'\n",
        "with open(tweet_train_json_file_path, 'r') as file:\n",
        "    tweet_train_data = json.load(file)\n",
        "tweet_test_json_file_path = '/content/drive/MyDrive/Common files/Common files/Dataset/Tweets/test.json'\n",
        "with open(tweet_test_json_file_path, 'r') as file:\n",
        "    tweet_test_data = json.load(file)\n",
        "tweet_valid_json_file_path = '/content/drive/MyDrive/Common files/Common files/Dataset/Tweets/valid.json'\n",
        "with open(tweet_valid_json_file_path, 'r') as file:\n",
        "    tweet_valid_data = json.load(file)"
      ],
      "metadata": {
        "id": "gPOZriEw9Y8d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "laptop_df_train = process_data(laptop_train_data)\n",
        "laptop_df_test = process_data(laptop_test_data)\n",
        "laptop_df_valid = process_data(laptop_valid_data)\n",
        "\n",
        "mams_df_train = process_data(mams_train_data)\n",
        "mams_df_test = process_data(mams_test_data)\n",
        "mams_df_valid = process_data(mams_valid_data)\n",
        "\n",
        "res_df_train = process_data(res_train_data)\n",
        "res_df_test = process_data(res_test_data)\n",
        "res_df_valid = process_data(res_valid_data)\n",
        "\n",
        "tweet_df_train = process_data(tweet_train_data)\n",
        "tweet_df_test = process_data(tweet_test_data)\n",
        "tweet_df_valid = process_data(tweet_valid_data)\n"
      ],
      "metadata": {
        "id": "u45MXN6p9eEu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Adjust tokenizer with a fixed maximum vocabulary size\n",
        "MAX_VOCAB_SIZE = 5000\n",
        "tokenizer = Tokenizer(num_words=MAX_VOCAB_SIZE, oov_token='<OOV>')\n",
        "# Set a fixed maximum sequence length\n",
        "max_seq_length = 200\n",
        "tag_to_index = {'O': 0, 'B': 1, 'I': 2,'PAD':3}\n",
        "embedding_dim = 200  # Adjust based on the GloVe file you downloaded\n",
        "embedding_matrix = np.zeros((len(tokenizer.word_index) + 1, embedding_dim))\n",
        "for word, i in tokenizer.word_index.items():\n",
        "    if word in word_vectors:\n",
        "        embedding_matrix[i] = word_vectors[word]"
      ],
      "metadata": {
        "id": "K7cnEJdC9eXM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def tokenize_padded_data(df_train, df_test, df_valid, tokenizer, tag_to_index, max_seq_length):\n",
        "    # Fit the tokenizer on the training data\n",
        "    tokenizer.fit_on_texts(df_train['token'])\n",
        "    tokenizer.fit_on_texts(df_train['pos'])\n",
        "\n",
        "    # Pad token sequences with the fixed maximum sequence length\n",
        "    X_train = pad_sequences(tokenizer.texts_to_sequences(df_train['token']), maxlen=max_seq_length, padding='post', truncating='post')\n",
        "    X_test = pad_sequences(tokenizer.texts_to_sequences(df_test['token']), maxlen=max_seq_length, padding='post', truncating='post')\n",
        "    X_val = pad_sequences(tokenizer.texts_to_sequences(df_valid['token']), maxlen=max_seq_length, padding='post', truncating='post')\n",
        "\n",
        "    # Pad POS sequences with the fixed maximum sequence length\n",
        "    pos_vocab_size = len(tokenizer.word_index) + 1  # Vocabulary size for POS tags\n",
        "    X_train_pos = pad_sequences(tokenizer.texts_to_sequences(df_train['pos']), maxlen=max_seq_length, padding='post', truncating='post')\n",
        "    X_test_pos = pad_sequences(tokenizer.texts_to_sequences(df_test['pos']), maxlen=max_seq_length, padding='post', truncating='post')\n",
        "    X_val_pos = pad_sequences(tokenizer.texts_to_sequences(df_valid['pos']), maxlen=max_seq_length, padding='post', truncating='post')\n",
        "\n",
        "    # Pad BIO tag sequences with the fixed maximum sequence length\n",
        "    y_train = pad_sequences([[tag_to_index[tag] for tag in seq] for seq in df_train['bioTag']], padding='post', value=tag_to_index['PAD'], maxlen=max_seq_length)\n",
        "    y_test = pad_sequences([[tag_to_index[tag] for tag in seq] for seq in df_test['bioTag']], padding='post', value=tag_to_index['PAD'], maxlen=max_seq_length)\n",
        "    y_val = pad_sequences([[tag_to_index[tag] for tag in seq] for seq in df_valid['bioTag']], padding='post', value=tag_to_index['PAD'], maxlen=max_seq_length)\n",
        "\n",
        "    # Convert numerical representations to one-hot encoding\n",
        "    y_train = to_categorical(y_train, num_classes=len(tag_to_index))\n",
        "    y_test = to_categorical(y_test, num_classes=len(tag_to_index))\n",
        "    y_val = to_categorical(y_val, num_classes=len(tag_to_index))\n",
        "\n",
        "    return X_train, X_test, X_val, X_train_pos, X_test_pos, X_val_pos, y_train, y_test, y_val, pos_vocab_size\n",
        "\n",
        "# Example usage:\n",
        "\n",
        "tokenizer = Tokenizer(num_words=MAX_VOCAB_SIZE, oov_token='<OOV>')\n",
        "\n",
        "# Assuming laptop_df_train, laptop_df_test, laptop_df_valid are your DataFrames for the laptop domain\n",
        "laptop_X_train, laptop_X_test, laptop_X_val, laptop_X_train_pos, laptop_X_test_pos, laptop_X_val_pos, laptop_y_train, laptop_y_test, laptop_y_val, pos_vocab_size = tokenize_padded_data(laptop_df_train, laptop_df_test, laptop_df_valid, tokenizer, tag_to_index, max_seq_length)\n",
        "res_X_train, res_X_test, res_X_val, res_X_train_pos, res_X_test_pos, res_X_val_pos, res_y_train, res_y_test, res_y_val, pos_vocab_size = tokenize_padded_data(res_df_train, res_df_test, res_df_valid, tokenizer,tag_to_index, max_seq_length)\n",
        "tweet_X_train, tweet_X_test, tweet_X_val, tweet_X_train_pos, tweet_X_test_pos, tweet_X_val_pos, tweet_y_train, tweet_y_test, tweet_y_val, pos_vocab_size = tokenize_padded_data(tweet_df_train, tweet_df_test, tweet_df_valid, tokenizer, tag_to_index, max_seq_length)\n",
        "mams_X_train, mams_X_test, mams_X_val, mams_X_train_pos, mams_X_test_pos, mams_X_val_pos, mams_y_train, mams_y_test, mams_y_val, pos_vocab_size = tokenize_padded_data(mams_df_train, mams_df_test, mams_df_valid, tokenizer, tag_to_index , max_seq_length)\n",
        "\n",
        "\n",
        "\n",
        "# Define L2 regularization strength\n",
        "l2_reg = 0.01\n",
        "\n",
        "# Update model architecture with regularization and class weights\n",
        "def create_model_with_position_embeddings(embedding_matrix, max_seq_length, max_pos_length):\n",
        "    # Input layers\n",
        "    token_input_layer = Input(shape=(max_seq_length,))\n",
        "    pos_input_layer = Input(shape=(max_seq_length,))\n",
        "\n",
        "    # Word embedding layer\n",
        "    word_embedding_layer = Embedding(input_dim=embedding_matrix.shape[0],\n",
        "                                     output_dim=embedding_matrix.shape[1],\n",
        "                                     input_length=max_seq_length,\n",
        "                                     weights=[embedding_matrix],\n",
        "                                     trainable=True, mask_zero=True)\n",
        "    token_embedding = word_embedding_layer(token_input_layer)\n",
        "\n",
        "    # Positional embedding layer\n",
        "    pos_embedding_layer = Embedding(input_dim=pos_vocab_size,\n",
        "                                    output_dim=50,\n",
        "                                    input_length=max_seq_length)\n",
        "    pos_embedding = pos_embedding_layer(pos_input_layer)\n",
        "\n",
        "    # Concatenate token embeddings and positional embeddings\n",
        "    combined_embedding = Concatenate(axis=-1)([token_embedding, pos_embedding])\n",
        "\n",
        "    # Bidirectional LSTM layer\n",
        "    lstm = Bidirectional(LSTM(units=100, return_sequences=True))()\n",
        "\n",
        "\n",
        "\n",
        "    # Feedforward layers with regularization\n",
        "    output = TimeDistributed(Dense(128, activation='tanh', kernel_regularizer=l2(l2_reg)))(lstm)\n",
        "    output = Dropout(0.1)(output)\n",
        "    output = TimeDistributed(Dense(4, activation='softmax'))(output)  # Assuming 3 classes: 'O', 'B', 'I'\n",
        "\n",
        "    # Create model\n",
        "    model = Model(inputs=[token_input_layer, pos_input_layer], outputs=output)\n",
        "\n",
        "    return model"
      ],
      "metadata": {
        "id": "8rWC02GR9imB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define static class weights\n",
        "  # Adjust weights based on the classification report\n",
        "\n",
        "print(\"########################################################## Laptop ########################################################\")\n",
        "laptop_model_with_position_embeddings = create_model_with_position_embeddings(embedding_matrix, max_seq_length, len(tag_to_index))\n",
        "laptop_model_with_position_embeddings.compile(optimizer=Adam(learning_rate=0.001), loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "laptop_early_stopping = EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)\n",
        "laptop_history = laptop_model_with_position_embeddings.fit([laptop_X_train, laptop_X_train_pos], laptop_y_train, validation_data=([laptop_X_val, laptop_X_val_pos], laptop_y_val), epochs=50,batch_size=32,callbacks=[laptop_early_stopping])\n",
        "\n",
        "print(\"########################################################## Restaurant ########################################################\")\n",
        "res_model_with_position_embeddings = create_model_with_position_embeddings(embedding_matrix, max_seq_length, len(tag_to_index))\n",
        "res_model_with_position_embeddings.compile(optimizer=Adam(learning_rate=0.001), loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "res_early_stopping = EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)\n",
        "res_history = res_model_with_position_embeddings.fit([res_X_train, res_X_train_pos], res_y_train, validation_data=([res_X_val, res_X_val_pos], res_y_val), epochs=50,batch_size=32,callbacks=[res_early_stopping])\n",
        "\n",
        "\n",
        "print(\"########################################################## Tweets ########################################################\")\n",
        "tweet_model_with_position_embeddings = create_model_with_position_embeddings(embedding_matrix, max_seq_length, len(tag_to_index))\n",
        "tweet_model_with_position_embeddings.compile(optimizer=Adam(learning_rate=0.001), loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "tweet_early_stopping = EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)\n",
        "tweet_history = tweet_model_with_position_embeddings.fit([tweet_X_train, tweet_X_train_pos], tweet_y_train, validation_data=([tweet_X_val, tweet_X_val_pos], tweet_y_val), epochs=50,batch_size=32,callbacks=[tweet_early_stopping])\n",
        "\n",
        "\n",
        "print(\"########################################################## MAMS ########################################################\")\n",
        "mams_model_with_position_embeddings = create_model_with_position_embeddings(embedding_matrix, max_seq_length, len(tag_to_index))\n",
        "mams_model_with_position_embeddings.compile(optimizer=Adam(learning_rate=0.001), loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "mams_early_stopping = EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)\n",
        "mams_history = mams_model_with_position_embeddings.fit([mams_X_train, mams_X_train_pos], mams_y_train, validation_data=([mams_X_val, mams_X_val_pos], mams_y_val), epochs=50,batch_size=32,callbacks=[mams_early_stopping])\n"
      ],
      "metadata": {
        "id": "dnXpkISpA8TR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "105afc4e-34fd-47cf-e6c2-69a9c9e51812"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "########################################################## Laptop ########################################################\n",
            "Epoch 1/50\n",
            "46/46 [==============================] - 27s 269ms/step - loss: 3.3207 - accuracy: 0.8463 - val_loss: 1.7473 - val_accuracy: 0.8201\n",
            "Epoch 2/50\n",
            "46/46 [==============================] - 5s 113ms/step - loss: 1.0667 - accuracy: 0.8663 - val_loss: 0.7483 - val_accuracy: 0.8201\n",
            "Epoch 3/50\n",
            "46/46 [==============================] - 3s 67ms/step - loss: 0.5197 - accuracy: 0.8645 - val_loss: 0.5005 - val_accuracy: 0.8300\n",
            "Epoch 4/50\n",
            "46/46 [==============================] - 3s 59ms/step - loss: 0.3911 - accuracy: 0.8700 - val_loss: 0.4283 - val_accuracy: 0.8450\n",
            "Epoch 5/50\n",
            "46/46 [==============================] - 2s 50ms/step - loss: 0.3480 - accuracy: 0.8743 - val_loss: 0.4051 - val_accuracy: 0.8450\n",
            "Epoch 6/50\n",
            "46/46 [==============================] - 3s 58ms/step - loss: 0.3322 - accuracy: 0.8766 - val_loss: 0.3944 - val_accuracy: 0.8463\n",
            "Epoch 7/50\n",
            "46/46 [==============================] - 2s 45ms/step - loss: 0.3196 - accuracy: 0.8774 - val_loss: 0.3876 - val_accuracy: 0.8458\n",
            "Epoch 8/50\n",
            "46/46 [==============================] - 1s 31ms/step - loss: 0.3177 - accuracy: 0.8765 - val_loss: 0.3860 - val_accuracy: 0.8479\n",
            "Epoch 9/50\n",
            "46/46 [==============================] - 1s 24ms/step - loss: 0.3122 - accuracy: 0.8792 - val_loss: 0.3907 - val_accuracy: 0.8460\n",
            "Epoch 10/50\n",
            "46/46 [==============================] - 1s 32ms/step - loss: 0.3132 - accuracy: 0.8772 - val_loss: 0.3850 - val_accuracy: 0.8419\n",
            "Epoch 11/50\n",
            "46/46 [==============================] - 1s 24ms/step - loss: 0.3117 - accuracy: 0.8780 - val_loss: 0.3777 - val_accuracy: 0.8513\n",
            "Epoch 12/50\n",
            "46/46 [==============================] - 1s 20ms/step - loss: 0.3085 - accuracy: 0.8794 - val_loss: 0.3714 - val_accuracy: 0.8499\n",
            "Epoch 13/50\n",
            "46/46 [==============================] - 1s 28ms/step - loss: 0.3072 - accuracy: 0.8817 - val_loss: 0.3802 - val_accuracy: 0.8438\n",
            "Epoch 14/50\n",
            "46/46 [==============================] - 1s 25ms/step - loss: 0.3035 - accuracy: 0.8823 - val_loss: 0.3733 - val_accuracy: 0.8486\n",
            "Epoch 15/50\n",
            "46/46 [==============================] - 1s 24ms/step - loss: 0.3045 - accuracy: 0.8816 - val_loss: 0.3908 - val_accuracy: 0.8443\n",
            "########################################################## Restaurant ########################################################\n",
            "Epoch 1/50\n",
            "62/62 [==============================] - 20s 178ms/step - loss: 2.8812 - accuracy: 0.8200 - val_loss: 1.2731 - val_accuracy: 0.8107\n",
            "Epoch 2/50\n",
            "62/62 [==============================] - 4s 58ms/step - loss: 0.7635 - accuracy: 0.8353 - val_loss: 0.5426 - val_accuracy: 0.8223\n",
            "Epoch 3/50\n",
            "62/62 [==============================] - 2s 38ms/step - loss: 0.4385 - accuracy: 0.8466 - val_loss: 0.4239 - val_accuracy: 0.8336\n",
            "Epoch 4/50\n",
            "62/62 [==============================] - 2s 33ms/step - loss: 0.3722 - accuracy: 0.8522 - val_loss: 0.3857 - val_accuracy: 0.8407\n",
            "Epoch 5/50\n",
            "62/62 [==============================] - 3s 53ms/step - loss: 0.3560 - accuracy: 0.8546 - val_loss: 0.3740 - val_accuracy: 0.8431\n",
            "Epoch 6/50\n",
            "62/62 [==============================] - 2s 26ms/step - loss: 0.3422 - accuracy: 0.8565 - val_loss: 0.3766 - val_accuracy: 0.8379\n",
            "Epoch 7/50\n",
            "62/62 [==============================] - 1s 20ms/step - loss: 0.3405 - accuracy: 0.8551 - val_loss: 0.3669 - val_accuracy: 0.8370\n",
            "Epoch 8/50\n",
            "62/62 [==============================] - 1s 20ms/step - loss: 0.3332 - accuracy: 0.8601 - val_loss: 0.3554 - val_accuracy: 0.8440\n",
            "Epoch 9/50\n",
            "62/62 [==============================] - 1s 19ms/step - loss: 0.3307 - accuracy: 0.8603 - val_loss: 0.3581 - val_accuracy: 0.8448\n",
            "Epoch 10/50\n",
            "62/62 [==============================] - 1s 21ms/step - loss: 0.3321 - accuracy: 0.8607 - val_loss: 0.3518 - val_accuracy: 0.8448\n",
            "Epoch 11/50\n",
            "62/62 [==============================] - 1s 21ms/step - loss: 0.3284 - accuracy: 0.8604 - val_loss: 0.3588 - val_accuracy: 0.8451\n",
            "Epoch 12/50\n",
            "62/62 [==============================] - 1s 21ms/step - loss: 0.3251 - accuracy: 0.8598 - val_loss: 0.3513 - val_accuracy: 0.8440\n",
            "Epoch 13/50\n",
            "62/62 [==============================] - 1s 21ms/step - loss: 0.3228 - accuracy: 0.8615 - val_loss: 0.3498 - val_accuracy: 0.8443\n",
            "Epoch 14/50\n",
            "62/62 [==============================] - 2s 34ms/step - loss: 0.3212 - accuracy: 0.8632 - val_loss: 0.3702 - val_accuracy: 0.8421\n",
            "Epoch 15/50\n",
            "62/62 [==============================] - 2s 28ms/step - loss: 0.3202 - accuracy: 0.8628 - val_loss: 0.3518 - val_accuracy: 0.8465\n",
            "Epoch 16/50\n",
            "62/62 [==============================] - 1s 20ms/step - loss: 0.3206 - accuracy: 0.8616 - val_loss: 0.3637 - val_accuracy: 0.8402\n",
            "########################################################## Tweets ########################################################\n",
            "Epoch 1/50\n",
            "190/190 [==============================] - 23s 71ms/step - loss: 1.2667 - accuracy: 0.8922 - val_loss: 0.3487 - val_accuracy: 0.8964\n",
            "Epoch 2/50\n",
            "190/190 [==============================] - 5s 26ms/step - loss: 0.3080 - accuracy: 0.8998 - val_loss: 0.2790 - val_accuracy: 0.9078\n",
            "Epoch 3/50\n",
            "190/190 [==============================] - 4s 21ms/step - loss: 0.2820 - accuracy: 0.9037 - val_loss: 0.2693 - val_accuracy: 0.9090\n",
            "Epoch 4/50\n",
            "190/190 [==============================] - 4s 19ms/step - loss: 0.2730 - accuracy: 0.9066 - val_loss: 0.2610 - val_accuracy: 0.9128\n",
            "Epoch 5/50\n",
            "190/190 [==============================] - 5s 26ms/step - loss: 0.2686 - accuracy: 0.9076 - val_loss: 0.2607 - val_accuracy: 0.9116\n",
            "Epoch 6/50\n",
            "190/190 [==============================] - 3s 17ms/step - loss: 0.2651 - accuracy: 0.9086 - val_loss: 0.2576 - val_accuracy: 0.9133\n",
            "Epoch 7/50\n",
            "190/190 [==============================] - 3s 16ms/step - loss: 0.2621 - accuracy: 0.9089 - val_loss: 0.2522 - val_accuracy: 0.9151\n",
            "Epoch 8/50\n",
            "190/190 [==============================] - 4s 20ms/step - loss: 0.2605 - accuracy: 0.9091 - val_loss: 0.2498 - val_accuracy: 0.9142\n",
            "Epoch 9/50\n",
            "190/190 [==============================] - 4s 20ms/step - loss: 0.2606 - accuracy: 0.9088 - val_loss: 0.2567 - val_accuracy: 0.9137\n",
            "Epoch 10/50\n",
            "190/190 [==============================] - 3s 17ms/step - loss: 0.2579 - accuracy: 0.9095 - val_loss: 0.2517 - val_accuracy: 0.9140\n",
            "Epoch 11/50\n",
            "190/190 [==============================] - 3s 17ms/step - loss: 0.2572 - accuracy: 0.9099 - val_loss: 0.2486 - val_accuracy: 0.9135\n",
            "Epoch 12/50\n",
            "190/190 [==============================] - 4s 23ms/step - loss: 0.2572 - accuracy: 0.9092 - val_loss: 0.2487 - val_accuracy: 0.9119\n",
            "Epoch 13/50\n",
            "190/190 [==============================] - 3s 18ms/step - loss: 0.2551 - accuracy: 0.9105 - val_loss: 0.2445 - val_accuracy: 0.9154\n",
            "Epoch 14/50\n",
            "190/190 [==============================] - 3s 15ms/step - loss: 0.2540 - accuracy: 0.9102 - val_loss: 0.2459 - val_accuracy: 0.9133\n",
            "Epoch 15/50\n",
            "190/190 [==============================] - 3s 16ms/step - loss: 0.2545 - accuracy: 0.9101 - val_loss: 0.2450 - val_accuracy: 0.9147\n",
            "Epoch 16/50\n",
            "190/190 [==============================] - 4s 23ms/step - loss: 0.2524 - accuracy: 0.9114 - val_loss: 0.2459 - val_accuracy: 0.9139\n",
            "########################################################## MAMS ########################################################\n",
            "Epoch 1/50\n",
            "135/135 [==============================] - 22s 102ms/step - loss: 1.6614 - accuracy: 0.8556 - val_loss: 0.4371 - val_accuracy: 0.8639\n",
            "Epoch 2/50\n",
            "135/135 [==============================] - 6s 42ms/step - loss: 0.3568 - accuracy: 0.8661 - val_loss: 0.3383 - val_accuracy: 0.8595\n",
            "Epoch 3/50\n",
            "135/135 [==============================] - 4s 26ms/step - loss: 0.3185 - accuracy: 0.8708 - val_loss: 0.3182 - val_accuracy: 0.8701\n",
            "Epoch 4/50\n",
            "135/135 [==============================] - 3s 22ms/step - loss: 0.3084 - accuracy: 0.8737 - val_loss: 0.3152 - val_accuracy: 0.8731\n",
            "Epoch 5/50\n",
            "135/135 [==============================] - 4s 27ms/step - loss: 0.3078 - accuracy: 0.8717 - val_loss: 0.3287 - val_accuracy: 0.8693\n",
            "Epoch 6/50\n",
            "135/135 [==============================] - 4s 27ms/step - loss: 0.3041 - accuracy: 0.8741 - val_loss: 0.3076 - val_accuracy: 0.8728\n",
            "Epoch 7/50\n",
            "135/135 [==============================] - 3s 20ms/step - loss: 0.3024 - accuracy: 0.8751 - val_loss: 0.3071 - val_accuracy: 0.8724\n",
            "Epoch 8/50\n",
            "135/135 [==============================] - 3s 20ms/step - loss: 0.3021 - accuracy: 0.8739 - val_loss: 0.3089 - val_accuracy: 0.8710\n",
            "Epoch 9/50\n",
            "135/135 [==============================] - 3s 19ms/step - loss: 0.3005 - accuracy: 0.8747 - val_loss: 0.3066 - val_accuracy: 0.8737\n",
            "Epoch 10/50\n",
            "135/135 [==============================] - 4s 28ms/step - loss: 0.2974 - accuracy: 0.8755 - val_loss: 0.3023 - val_accuracy: 0.8747\n",
            "Epoch 11/50\n",
            "135/135 [==============================] - 3s 21ms/step - loss: 0.2989 - accuracy: 0.8752 - val_loss: 0.3045 - val_accuracy: 0.8764\n",
            "Epoch 12/50\n",
            "135/135 [==============================] - 2s 17ms/step - loss: 0.2974 - accuracy: 0.8759 - val_loss: 0.3069 - val_accuracy: 0.8741\n",
            "Epoch 13/50\n",
            "135/135 [==============================] - 3s 19ms/step - loss: 0.2978 - accuracy: 0.8741 - val_loss: 0.3040 - val_accuracy: 0.8758\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import classification_report, accuracy_score\n",
        "\n",
        "# Make predictions on the test set\n",
        "laptop_y_pred = laptop_model_with_position_embeddings.predict([laptop_X_test, laptop_X_test_pos])\n",
        "res_y_pred = res_model_with_position_embeddings.predict([res_X_test, res_X_test_pos])\n",
        "tweet_y_pred = tweet_model_with_position_embeddings.predict([tweet_X_test, tweet_X_test_pos])\n",
        "mams_y_pred = mams_model_with_position_embeddings.predict([mams_X_test, mams_X_test_pos])\n",
        "\n",
        "# Convert predictions and true labels from one-hot encoding to labels\n",
        "laptop_y_pred_labels = np.argmax(laptop_y_pred, axis=-1)\n",
        "laptop_y_test_labels = np.argmax(laptop_y_test, axis=-1)\n",
        "\n",
        "res_y_pred_labels = np.argmax(res_y_pred, axis=-1)\n",
        "res_y_test_labels = np.argmax(res_y_test, axis=-1)\n",
        "\n",
        "tweet_y_pred_labels = np.argmax(tweet_y_pred, axis=-1)\n",
        "tweet_y_test_labels = np.argmax(tweet_y_test, axis=-1)\n",
        "\n",
        "mams_y_pred_labels = np.argmax(mams_y_pred, axis=-1)\n",
        "mams_y_test_labels = np.argmax(mams_y_test, axis=-1)\n",
        "\n",
        "# Flatten the predictions and true labels to prepare for classification_report\n",
        "laptop_y_pred_labels_flat = laptop_y_pred_labels.flatten()\n",
        "laptop_y_test_labels_flat = laptop_y_test_labels.flatten()\n",
        "\n",
        "res_y_pred_labels_flat = res_y_pred_labels.flatten()\n",
        "res_y_test_labels_flat = res_y_test_labels.flatten()\n",
        "\n",
        "tweet_y_pred_labels_flat = tweet_y_pred_labels.flatten()\n",
        "tweet_y_test_labels_flat = tweet_y_test_labels.flatten()\n",
        "\n",
        "mams_y_pred_labels_flat = mams_y_pred_labels.flatten()\n",
        "mams_y_test_labels_flat = mams_y_test_labels.flatten()\n",
        "\n",
        "\n",
        "# Generate classification report\n",
        "class_names = ['O', 'B', 'I']\n",
        "# Filter out 'PAD' tag from predictions and true labels\n",
        "laptop_non_pad_indices = laptop_y_test_labels_flat != tag_to_index['PAD']\n",
        "laptop_y_pred_labels_filtered = laptop_y_pred_labels_flat[laptop_non_pad_indices]\n",
        "laptop_y_test_labels_filtered = laptop_y_test_labels_flat[laptop_non_pad_indices]\n",
        "\n",
        "res_non_pad_indices = res_y_test_labels_flat != tag_to_index['PAD']\n",
        "res_y_pred_labels_filtered = res_y_pred_labels_flat[res_non_pad_indices]\n",
        "res_y_test_labels_filtered = res_y_test_labels_flat[res_non_pad_indices]\n",
        "\n",
        "tweet_non_pad_indices = tweet_y_test_labels_flat != tag_to_index['PAD']\n",
        "tweet_y_pred_labels_filtered = tweet_y_pred_labels_flat[tweet_non_pad_indices]\n",
        "tweet_y_test_labels_filtered = tweet_y_test_labels_flat[tweet_non_pad_indices]\n",
        "\n",
        "mams_non_pad_indices = mams_y_test_labels_flat != tag_to_index['PAD']\n",
        "mams_y_pred_labels_filtered = mams_y_pred_labels_flat[mams_non_pad_indices]\n",
        "mams_y_test_labels_filtered = mams_y_test_labels_flat[mams_non_pad_indices]\n",
        "\n",
        "# Generate classification report\n",
        "laptop_report = classification_report(laptop_y_test_labels_filtered, laptop_y_pred_labels_filtered, target_names=class_names)\n",
        "res_report = classification_report(res_y_test_labels_filtered, res_y_pred_labels_filtered, target_names=class_names)\n",
        "tweet_report = classification_report(tweet_y_test_labels_filtered, tweet_y_pred_labels_filtered, target_names=class_names)\n",
        "mams_report = classification_report(mams_y_test_labels_filtered, mams_y_pred_labels_filtered, target_names=class_names)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Print classification report\n",
        "print(\"##########################################################  Laptop  #############################################\")\n",
        "print(\"Laptop Classification Report (excluding PAD tag):\")\n",
        "print(laptop_report)\n",
        "laptop_accuracy = accuracy_score(laptop_y_test_labels_filtered, laptop_y_pred_labels_filtered)\n",
        "print(\"Laptop Accuracy (excluding PAD tag):\", laptop_accuracy)\n",
        "\n",
        "# Print classification report\n",
        "print(\"##########################################################  Restaurants  #############################################\")\n",
        "print(\"Restaurants Classification Report (excluding PAD tag):\")\n",
        "print(res_report)\n",
        "res_accuracy = accuracy_score(res_y_test_labels_filtered, res_y_pred_labels_filtered)\n",
        "print(\"Restaurants Accuracy (excluding PAD tag):\", res_accuracy)\n",
        "\n",
        "# Print classification report\n",
        "print(\"##########################################################  Mams  #############################################\")\n",
        "print(\"MAMS Classification Report (excluding PAD tag):\")\n",
        "print(mams_report)\n",
        "# Calculate accuracy excluding 'PAD' tag\n",
        "mams_accuracy = accuracy_score(mams_y_test_labels_filtered, mams_y_pred_labels_filtered)\n",
        "print(\"MAMS Accuracy (excluding PAD tag):\", mams_accuracy)\n",
        "\n",
        "# Print classification report\n",
        "print(\"########################################################## Tweet  #############################################\")\n",
        "print(\"Tweet Classification Report (excluding PAD tag):\")\n",
        "print(tweet_report)\n",
        "# Calculate accuracy excluding 'PAD' tag\n",
        "tweet_accuracy = accuracy_score(tweet_y_test_labels_filtered, tweet_y_pred_labels_filtered)\n",
        "print(\"Laptop Accuracy (excluding PAD tag):\", tweet_accuracy)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9NLuG7gdVWHq",
        "outputId": "27d13d1b-6717-4630-a7af-bf31c1ee449b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "13/13 [==============================] - 2s 6ms/step\n",
            "19/19 [==============================] - 3s 8ms/step\n",
            "22/22 [==============================] - 3s 6ms/step\n",
            "16/16 [==============================] - 2s 7ms/step\n",
            "##########################################################  Laptop  #############################################\n",
            "Laptop Classification Report (excluding PAD tag):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           O       0.88      0.95      0.92      4776\n",
            "           B       0.57      0.33      0.41       623\n",
            "           I       0.62      0.47      0.53       425\n",
            "\n",
            "    accuracy                           0.85      5824\n",
            "   macro avg       0.69      0.58      0.62      5824\n",
            "weighted avg       0.83      0.85      0.83      5824\n",
            "\n",
            "Laptop Accuracy (excluding PAD tag): 0.8499313186813187\n",
            "##########################################################  Restaurants  #############################################\n",
            "Restaurants Classification Report (excluding PAD tag):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           O       0.91      0.92      0.92      7012\n",
            "           B       0.57      0.58      0.58      1103\n",
            "           I       0.48      0.36      0.41       534\n",
            "\n",
            "    accuracy                           0.84      8649\n",
            "   macro avg       0.65      0.62      0.63      8649\n",
            "weighted avg       0.84      0.84      0.84      8649\n",
            "\n",
            "Restaurants Accuracy (excluding PAD tag): 0.8442594519597642\n",
            "##########################################################  Mams  #############################################\n",
            "MAMS Classification Report (excluding PAD tag):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           O       0.90      0.97      0.93     10479\n",
            "           B       0.60      0.43      0.50      1323\n",
            "           I       0.27      0.01      0.01       401\n",
            "\n",
            "    accuracy                           0.88     12203\n",
            "   macro avg       0.59      0.47      0.48     12203\n",
            "weighted avg       0.85      0.88      0.85     12203\n",
            "\n",
            "MAMS Accuracy (excluding PAD tag): 0.8755224125215111\n",
            "########################################################## Tweet  #############################################\n",
            "Tweet Classification Report (excluding PAD tag):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           O       0.92      0.99      0.96     10707\n",
            "           B       0.75      0.22      0.34       716\n",
            "           I       0.76      0.30      0.43       521\n",
            "\n",
            "    accuracy                           0.92     11944\n",
            "   macro avg       0.81      0.50      0.57     11944\n",
            "weighted avg       0.90      0.92      0.90     11944\n",
            "\n",
            "Laptop Accuracy (excluding PAD tag): 0.9154387139986604\n"
          ]
        }
      ]
    }
  ]
}